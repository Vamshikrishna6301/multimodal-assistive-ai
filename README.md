# ğŸ§  Multimodal Assistive AI Personal Assistant
### Voice â€¢ Vision â€¢ Gesture â€¢ Context â€¢ Safety â€¢ Adaptation

> A **voice-first, decision-driven multimodal personal assistant** designed especially for differently-abled users  
> Built phase-by-phase with **low latency, safety, explainability, and modularity** as core principles.

This is **NOT a chatbot**.  
This is an **assistive AI system** that safely controls a computer and environment.

---

## ğŸ“Œ CORE DESIGN PHILOSOPHY

Multiple Inputs â†’ One Decision Engine â†’ One Safe Action


- Inputs: Voice, Vision, Gesture, Emotion, Context
- Intelligence: Rule-based + ML-assisted (hybrid AI)
- Safety > Intelligence
- LLMs are helpers, never controllers

---

## ğŸ“ COMPLETE FILE STRUCTURE (CURRENT + FUTURE)

KRISHNA/
â”‚
â”œâ”€â”€ voice/ # Phase 1
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ mic_stream.py # Microphone streaming
â”‚ â”œâ”€â”€ vad.py # Voice Activity Detection
â”‚ â”œâ”€â”€ stt.py # Speech-to-Text (Whisper)
â”‚ â”œâ”€â”€ tts.py # Text-to-Speech (edge-tts)
â”‚ â”œâ”€â”€ wakeword.py # Fuzzy wake word detection
â”‚ â””â”€â”€ voice_loop.py # Full duplex voice loop
â”‚
â”œâ”€â”€ core/ # Phase 2, 5, 8
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ intent_schema.py # Intent dataclasses
â”‚ â”œâ”€â”€ intent_parser.py # Rule-based intent parsing
â”‚ â”œâ”€â”€ mode_manager.py # COMMAND / DICTATION / QUESTION
â”‚ â”œâ”€â”€ safety_rules.py # Confirmation & blocking
â”‚ â”œâ”€â”€ context_memory.py # Session memory
â”‚ â”œâ”€â”€ intent_buffer.py # Multimodal buffering
â”‚ â”œâ”€â”€ priority_rules.py # Conflict resolution
â”‚ â””â”€â”€ fusion_engine.py # Phase 8 core
â”‚
â”œâ”€â”€ execution/ # Phase 3
â”‚ â”œâ”€â”€ app_control.py
â”‚ â”œâ”€â”€ keyboard_mouse.py
â”‚ â””â”€â”€ file_ops.py
â”‚
â”œâ”€â”€ vision/ # Phase 4
â”‚ â”œâ”€â”€ screen_capture.py
â”‚ â”œâ”€â”€ camera_capture.py
â”‚ â”œâ”€â”€ ocr_reader.py
â”‚ â””â”€â”€ object_detection.py
â”‚
â”œâ”€â”€ gesture/ # Phase 6
â”‚ â”œâ”€â”€ hand_tracker.py
â”‚ â””â”€â”€ gesture_rules.py
â”‚
â”œâ”€â”€ emotion/ # Phase 7
â”‚ â”œâ”€â”€ face_emotion.py
â”‚ â””â”€â”€ voice_stress.py
â”‚
â”œâ”€â”€ learning/ # Phase 9
â”‚ â””â”€â”€ adaptive_rules.py
â”‚
â”œâ”€â”€ ui/ # Phase 10
â”‚ â””â”€â”€ dashboard.py
â”‚
â”œâ”€â”€ config.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸŸ¢ PHASE 1 â€” VOICE I/O FOUNDATION  
**Status: âœ… COMPLETED (CPU stable)**

### Goal
Build a **real-time, low-latency, full-duplex voice pipeline**

### Features
- Microphone streaming
- Voice Activity Detection (VAD)
- Speech-to-Text (Whisper)
- Text-to-Speech (Windows-safe)
- Wake-word based activation
- Noise tolerance
- Continuous listening

### Functions / Modules
- `MicrophoneStream.read()`
- `VAD.is_speech()`
- `STT.transcribe()`
- `TTS.speak()`
- `is_wake_word()`
- `VoiceLoop.run()`

### Tech Stack
- `sounddevice`
- `webrtcvad`
- `faster-whisper` (CPU)
- `edge-tts`
- `numpy`

### Test Cases
| Test | Expected |
|----|----|
Silence | No output |
Noise | Ignored |
Wake word | Voice reply |
Repeated wake word | Responds |
Long run | No crash |

---

## ğŸŸ¡ PHASE 2 â€” INTENT & MODE ENGINE  
**Status: â³ NEXT**

### Goal
Understand **what kind of input** the user gave

### Features
- Intent schema (dataclasses)
- Rule-based intent parsing
- Modes:
  - COMMAND
  - DICTATION
  - QUESTION
  - DISABLED
- Safety confirmations

### Functions / Modules
- `Intent(type, action, target, params)`
- `parse_intent(text)`
- `ModeManager.set_mode()`
- `SafetyRules.requires_confirmation()`

### Tech Stack
- Python `dataclasses`
- Regex / keyword rules
- Finite state machine

### Test Cases
| Input | Result |
|----|----|
Open Chrome | COMMAND |
Type hello | DICTATION |
Delete all files | Confirmation |
Disable assistant | Ignored |

---

## ğŸŸ¡ PHASE 3 â€” TASK EXECUTION ENGINE  
**Status: â³ PLANNED**

### Goal
Execute **real OS actions** safely

### Features
- Open / close applications
- Browser search & playback
- Keyboard automation
- Mouse automation
- File operations

### Functions / Modules
- `open_app()`
- `search_browser()`
- `type_text()`
- `scroll()`
- `close_active_app()`

### Tech Stack
- `pyautogui`
- `subprocess`
- `os`, `platform`

### Test Cases
| Command | Result |
|----|----|
Open Chrome | Browser opens |
Search today score | Results shown |
Play YouTube video | Video plays |
Close it | App closes |

---

## ğŸŸ¡ PHASE 4 â€” VISION â†’ VOICE  
**Status: â³ PLANNED**

### Goal
Describe **screen & surroundings** via voice

### Features
- Screen capture
- Camera capture
- OCR reading
- Object detection
- Spoken narration

### Functions / Modules
- `capture_screen()`
- `read_text_from_screen()`
- `detect_objects()`
- `narrate_scene()`

### Tech Stack
- `OpenCV`
- `Tesseract OCR`
- `YOLOv8`
- GPU (RTX 2050)

### Test Cases
| Query | Result |
|----|----|
What is on my screen | Spoken summary |
Read this page | OCR + TTS |
Is there a button | Spatial answer |

---

## ğŸŸ¡ PHASE 5 â€” CONTEXT MEMORY  
**Status: â³ PLANNED**

### Goal
Make interaction **context-aware**

### Features
- Session memory
- Reference resolution
- Action repetition
- Error recovery

### Functions / Modules
- `store_last_action()`
- `resolve_reference("it")`
- `repeat_last_action()`

### Tech Stack
- Python dict / deque
- Optional SQLite

### Test Cases
| Input | Result |
|----|----|
Close it | Closes last app |
Do that again | Repeats action |

---

## ğŸŸ¡ PHASE 6 â€” GESTURE INTERACTION  
**Status: â³ PLANNED**

### Goal
Enable **non-speaking users** and safety overrides

### Features
- Hand detection
- Simple symbolic gestures
- Emergency stop

### Functions / Modules
- `detect_hand()`
- `classify_gesture()`
- `gesture_override()`

### Tech Stack
- `MediaPipe Hands`
- `OpenCV`

### Test Cases
| Gesture | Result |
|----|----|
âœ‹ Palm | Stop all actions |
ğŸ‘ Confirm | Execute command |

---

## ğŸŸ¡ PHASE 7 â€” EMOTION AWARENESS  
**Status: â³ PLANNED**

### Goal
Adapt behavior based on **user state**

### Features
- Facial emotion detection
- Voice stress analysis
- Cognitive load handling

### Functions / Modules
- `detect_emotion()`
- `analyze_voice_stress()`
- `modulate_response()`

### Tech Stack
- `MediaPipe Face Mesh`
- CNN (FER-2013)
- Audio prosody analysis

### Test Cases
| Condition | Behavior |
|----|----|
Stress + delete | Confirmation |
Fatigue | Short answers |

---

## ğŸ”´ PHASE 8 â€” MULTIMODAL FUSION ENGINE (CORE)  
**Status: â³ PLANNED (MOST IMPORTANT)**

### Goal
Resolve conflicts and ensure **single safe action**

### Features
- Intent buffer
- Priority rules
- Mode enforcement
- Emotion-aware suppression
- Single-action guarantee

### Functions / Modules
- `add_intent()`
- `resolve_conflicts()`
- `select_final_action()`

### Tech Stack
- Pure Python logic
- Rule engine
- Optional ML later

### Test Cases
| Inputs | Result |
|----|----|
Voice delete + stress | Block |
Voice yes + gesture stop | Cancel |
Multiple inputs | One action |

---

## ğŸŸ¡ PHASE 9 â€” ADAPTIVE LEARNING  
**Status: â³ PLANNED**

### Goal
Personalize assistant behavior

### Features
- Learn command preferences
- Learn confirmation tolerance
- Learn TTS speed

### Functions / Modules
- `update_preferences()`
- `adjust_tts_speed()`

### Tech Stack
- Rule-based learning
- Contextual bandits (optional)

### Test Cases
| Pattern | Result |
|----|----|
User says â€œbrowserâ€ | Opens Chrome |
Repeated confirmations | Removed |

---

## ğŸŸ¡ PHASE 10 â€” UI & ACCESSIBILITY PROFILES  
**Status: â³ PLANNED**

### Goal
Make system usable & demo-ready

### Features
- Minimal UI
- Voice-only mode
- Gesture-only mode
- Accessibility profiles

### Tech Stack
- `Tkinter` / `PyQt` / Web UI

### Test Cases
| Mode | Behavior |
|----|----|
Voice-only | No UI needed |
Gesture-only | Visual feedback |

---

## ğŸ FINAL NOTE

This project is:
- Major-project worthy
- Research & paper ready
- Resume flagship
- Assistive-technology focused

**Phase 1 is complete and stable.**  
Next development starts from **Phase 2 (Intent & Mode Engine)**.

---

## ğŸ¤ CONTRIBUTION GUIDE

1. Clone repo
2. Create virtual environment
3. Run Phase-1
4. Implement next phase in order
5. Do NOT skip phases

---

> â€œBuild intelligence only after safety is guaranteed.â€