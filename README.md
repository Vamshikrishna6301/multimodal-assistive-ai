Project Vision

This project is an AI-powered multimodal personal assistant tailored for differently-abled individuals, including:

Visually impaired users

Motor-disabled users

Speech-impaired users

Users with limited mobility or coordination

Users requiring adaptive interaction models

The system enables natural, intuitive, and safe interaction with both digital and physical environments using:

ğŸ¤ Voice input and output

âœ‹ Gesture recognition

ğŸ‘ Vision-based perception

ğŸ˜Š Emotion awareness

ğŸ§  Context-aware reasoning

ğŸ¯ Core Objective

To bridge the gap between human intent and machine execution by building a deterministic, safety-aware, multimodal assistive intelligence system that:

Reduces dependency on traditional input devices (keyboard, mouse)

Provides accessible computing interfaces

Ensures safe automation through confirmation safeguards

Maintains contextual awareness across interactions

Supports adaptive interaction based on user capability

ğŸ” Foundational Design Principles

The system is built on the following non-negotiable guarantees:

Deterministic decision logic

Explicit confirmation for high-risk actions

Single-action safe execution model

Context retention for multi-step interaction

Accessibility-first design

Safety over speed

Structured audit logging for transparency

ğŸ— High-Level System Architecture
Voice / Gesture / Vision / Emotion Inputs
                  â†“
           Intent Parsing Layer
                  â†“
           Context Memory
                  â†“
           Safety Evaluation
                  â†“
           Fusion Engine
                  â†“
           Decision Routing
                  â†“
           Execution Engine
                  â†“
           Adaptive User Feedback



           project_root/
â”‚
â”œâ”€â”€ core/                          # Phase 1 â€” Intent & Safety Core
â”‚   â”œâ”€â”€ context_memory.py
â”‚   â”œâ”€â”€ fusion_engine.py
â”‚   â”œâ”€â”€ intent_parser.py
â”‚   â”œâ”€â”€ intent_schema.py
â”‚   â”œâ”€â”€ mode_manager.py
â”‚   â”œâ”€â”€ safety_engine.py
â”‚   â”œâ”€â”€ safety_rules.py
â”‚   â””â”€â”€ response_model.py
â”‚
â”œâ”€â”€ router/                        # Phase 3 â€” Decision Routing
â”‚   â””â”€â”€ decision_router.py
â”‚
â”œâ”€â”€ execution/                     # Phase 3.1 â€” Execution Engine
â”‚   â”œâ”€â”€ executor.py
â”‚   â”œâ”€â”€ dispatcher.py
â”‚   â”œâ”€â”€ execution_logger.py
â”‚   â””â”€â”€ adapters/
â”‚       â”œâ”€â”€ windows_app.py
â”‚       â”œâ”€â”€ windows_browser.py
â”‚       â”œâ”€â”€ windows_keyboard.py
â”‚       â”œâ”€â”€ windows_file.py
â”‚       â””â”€â”€ windows_system.py
â”‚
â”œâ”€â”€ utility/                       # Phase 3.2 â€” Utility Engine
â”‚   â””â”€â”€ utility_engine.py
â”‚
â”œâ”€â”€ knowledge/                     # Phase 3.3 â€” Hybrid Knowledge
â”‚   â”œâ”€â”€ knowledge_engine.py
â”‚   â””â”€â”€ llm_engine.py
â”‚
â”œâ”€â”€ voice/                         # Phase 2 â€” Real-Time Runtime
â”‚   â”œâ”€â”€ assistant_runtime.py
â”‚   â”œâ”€â”€ mic_stream.py
â”‚   â”œâ”€â”€ vad.py
â”‚   â”œâ”€â”€ stt.py
â”‚   â”œâ”€â”€ tts.py
â”‚   â””â”€â”€ voice_loop.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_context.py
â”‚   â”œâ”€â”€ test_parser.py
â”‚   â”œâ”€â”€ test_safety.py
â”‚   â”œâ”€â”€ test_execution.py
â”‚   â””â”€â”€ test_voice_pipeline.py
â”‚
â”œâ”€â”€ demos/
â”‚   â”œâ”€â”€ demo_phase2.py
â”‚   â””â”€â”€ demo_full_pipeline.py
â”‚
â”œâ”€â”€ config.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸŸ¢ PHASE 1 â€” Core Intent & Safety Engine

Status: âœ… Complete

Goal

Create a deterministic, risk-aware decision engine capable of safely interpreting user intent.

Key Features

Structured Intent modeling (confidence, risk level, confirmation flags)

Flexible natural language parsing

Context memory for reference resolution

Mode-based interaction control

Hard blocking for unsafe operations

Latency tracking

Deterministic approval flow

Technologies Used

Python 3.x

Dataclasses

Threading

Regular Expressions

Structured JSON logging

Test Coverage

Command recognition accuracy

Risk escalation handling

Confirmation enforcement

Context resolution (â€œclose itâ€)

Mode switching validation

Unknown input blocking

ğŸ¤ PHASE 2 â€” Real-Time Voice Runtime

Status: âœ… Complete

Goal

Enable hands-free operation through robust, real-time speech processing.

Technologies

Faster-Whisper (GPU STT)

PyTorch CUDA

WebRTC VAD

SoundDevice

NumPy

Offline TTS (PowerShell / pyttsx3)

Multi-threaded runtime architecture

Capabilities

Real-time microphone streaming (16kHz)

Silence-based segmentation

Noise filtering

Echo mitigation

Non-blocking TTS

Confirmation voice loop

Runtime state management

Accessibility Impact

Enables blind and low-mobility users to operate systems without physical input devices.

Allows hands-free navigation and application control.

ğŸŸ¡ PHASE 3 â€” Execution & Knowledge Integration

Status: ğŸŸ¡ Execution Complete | Runtime Hardening Ongoing

Goal

Safely connect approved intents to real-world system actions.

3.1 Execution Engine
Features

Approved-only execution

Confirmation-required enforcement

OS abstraction via adapter layer

App stack tracking

Safe shutdown & restart controls

Structured execution logging

Logging System

Each action logs:

Timestamp

Action

Target

Risk level

Confirmation state

Success / Failure

Error code

Spoken response

Audit-ready.

3.2 Utility Engine

Handles:

Mathematical calculations

System time queries

Deterministic lightweight queries

3.3 Hybrid Knowledge Engine

Combines:

Wikipedia API (fast factual queries)

Local LLM (TinyLlama via Ollama) for reasoning

Features

Clean output formatting

Disambiguation filtering

Factual summarization (max 2 sentences)

LLM prompt hardening

No conversational filler

Offline capability (LLM)

ğŸ”µ PHASE 4 â€” Vision Integration (Planned)

Status: ğŸŸ¦ Planned

Objective

Enable visual perception for blind and visually impaired users.

Planned Capabilities

Screen capture

OCR text extraction

Object detection (real-world camera)

Scene narration

Environmental awareness

Technologies

OpenCV

Tesseract OCR

YOLOv8

NumPy

Accessibility Impact

Describes surroundings

Reads text from screen or environment

Assists in navigation

ğŸŸ£ PHASE 5 â€” Advanced Context Engine (Planned)

Multi-step task chaining

Task continuation memory

Reference resolution graph

Intelligent action linking

ğŸŸ  PHASE 6 â€” Gesture Interaction (Planned)

MediaPipe Hands

Gesture-to-command mapping

Cursor control

Emergency stop gesture

Override capability

Accessibility impact:
Enables interaction for users unable to speak clearly.

ğŸ”´ PHASE 7 â€” Emotion Awareness (Planned)

Facial emotion detection

Voice stress analysis

Adaptive response tone

Confirmation sensitivity adjustment

Accessibility impact:
Improves interaction comfort and reduces cognitive load.

ğŸŸ¡ PHASE 8 â€” Multimodal Fusion Core (Critical Phase)

Goal:

Resolve conflicts between:

Voice

Gesture

Vision

Emotion

Guarantee:

Exactly one safe action executes at a time.

Implements modality prioritization and confidence arbitration.

ğŸŸ¡ PHASE 9 â€” Adaptive Learning

Personalized shortcuts

Usage pattern modeling

Confirmation tolerance adaptation

Preference memory

ğŸŸ¡ PHASE 10 â€” Accessibility Profiles & UI Layer

Voice-only mode

Gesture-only mode

High-contrast dashboard

Slow-response mode

Low-motor configuration

Feedback customization

ğŸŒ Real-World Applications

Assistive computing for differently-abled individuals

Hospital bedside interaction systems

Smart home accessibility

Hands-free industrial control

Safety-critical environments

Accessibility research platforms

ğŸ Current Development Status
Phase	Status
Phase 1 â€” Core Engine	âœ… Complete
Phase 2 â€” Voice Runtime	âœ… Complete
Phase 3 â€” Execution & Knowledge	ğŸŸ¡ Stable
Phase 4 â€” Vision	ğŸŸ¦ Planned
Phase 5 â€” Advanced Context	ğŸŸ£ Planned
Phase 6 â€” Gesture	ğŸŸ  Planned
Phase 7 â€” Emotion	ğŸ”´ Planned
Phase 8 â€” Multimodal Fusion	ğŸŸ¡ Critical
Phase 9 â€” Adaptive Learning	ğŸŸ¡ Planned
Phase 10 â€” Accessibility UI	ğŸŸ¡ Planned