Project Vision

This project is an AI-powered multimodal personal assistant tailored for differently-abled individuals, including:

Visually impaired users

Motor-disabled users

Speech-impaired users

Users with limited mobility or coordination

Users requiring adaptive interaction models

The system enables natural, intuitive, and safe interaction with both digital and physical environments using:

ðŸŽ¤ Voice input and output

âœ‹ Gesture recognition

ðŸ‘ Vision-based perception

ðŸ˜Š Emotion awareness

ðŸ§  Context-aware reasoning

ðŸŽ¯ Core Objective

To bridge the gap between human intent and machine execution by building a deterministic, safety-aware, multimodal assistive intelligence system that:

Reduces dependency on traditional input devices (keyboard, mouse)

Provides accessible computing interfaces

Ensures safe automation through confirmation safeguards

Maintains contextual awareness across interactions

Supports adaptive interaction based on user capability

ðŸ” Foundational Design Principles

The system is built on the following non-negotiable guarantees:

Deterministic decision logic

Explicit confirmation for high-risk actions

Single-action safe execution model

Context retention for multi-step interaction

Accessibility-first design

Safety over speed

Structured audit logging for transparency

ðŸ— High-Level System Architecture
Voice / Gesture / Vision / Emotion Inputs
                  â†“
           Intent Parsing Layer
                  â†“
           Context Memory
                  â†“
           Safety Evaluation
                  â†“
           Fusion Engine
                  â†“
           Decision Routing
                  â†“
           Execution Engine
                  â†“
           Adaptive User Feedback


 KRISHNA/
â”œâ”€â”€ __pycache__/
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ context_memory.py
â”‚   â”œâ”€â”€ fusion_engine.py
â”‚   â”œâ”€â”€ intent_parser.py
â”‚   â”œâ”€â”€ intent_schema.py
â”‚   â”œâ”€â”€ mode_manager.py
â”‚   â”œâ”€â”€ response_model.py
â”‚   â”œâ”€â”€ safety_engine.py
â”‚   â””â”€â”€ safety_rules.py
â”‚
â”œâ”€â”€ execution/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   â”œâ”€â”€ windows_app.py
â”‚   â”‚   â”œâ”€â”€ windows_browser.py
â”‚   â”‚   â”œâ”€â”€ windows_file.py
â”‚   â”‚   â”œâ”€â”€ windows_keyboard.py
â”‚   â”‚   â””â”€â”€ windows_system.py
â”‚   â”‚
â”‚   â”œâ”€â”€ vision/
â”‚   â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ camera_detector.py
â”‚   â”‚   â”œâ”€â”€ ocr_engine.py
â”‚   â”‚   â”œâ”€â”€ screen_capture.py
â”‚   â”‚   â”œâ”€â”€ vision_executor.py
â”‚   â”‚   â””â”€â”€ app_control.py
â”‚   â”‚
â”‚   â”œâ”€â”€ dispatcher.py
â”‚   â”œâ”€â”€ execution_logger.py
â”‚   â”œâ”€â”€ execution.py
â”‚   â”œâ”€â”€ file_ops.py
â”‚   â””â”€â”€ keyboard_mouse.py
â”‚
â”œâ”€â”€ knowledge/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ knowledge_engine.py
â”‚   â””â”€â”€ llm_engine.py
â”‚
â”œâ”€â”€ router/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â””â”€â”€ decision_router.py
â”‚
â”œâ”€â”€ utility/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â””â”€â”€ utility_engine.py
â”‚
â”œâ”€â”€ voice/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ assistant_runtime.py
â”‚   â”œâ”€â”€ mic_stream.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ test_execution_vision.py
â”‚   â”œâ”€â”€ test_router_vision.py
â”‚   â””â”€â”€ test_vision_parser.py
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md          

PHASE 4 â€” Vision Integration
Status: ðŸŸ¢ Core Integrated | Runtime Optimized | Production Stable
ðŸŽ¯ Goal

Enable visual perception capabilities for:

Blind users

Low-vision users

Environmental awareness

Screen reading

Real-time object detection

ðŸ”¥ What We Built in This Chat
1ï¸âƒ£ Screen Capture System

File:

execution/vision/screen_capture.py

Capabilities:

Full screen capture

Compatible with Windows

Used for OCR pipeline

2ï¸âƒ£ OCR Engine (Text Reading)

File:

execution/vision/ocr_engine.py

Uses:

Tesseract OCR

Image preprocessing

Noise filtering

Supports:

â€œread what is on my screenâ€

Screen text narration

3ï¸âƒ£ Live Camera Object Detection (YOLOv8)

File:

execution/vision/camera_detector.py

Major Features Added:

Threaded non-blocking camera loop

CPU-based YOLO inference

Frame skipping (performance balance)

Confidence filtering (>= 0.5)

Stable speech summary every 2 sec

Bounding box drawing

Clean stop mechanism

STOP_CAMERA intent

Exit-safe shutdown

ðŸ§  Major Architecture Decisions (Very Important)
ðŸ”¥ GPU Allocation Strategy

You have:
RTX 2050 (4GB)

We designed:

Component	Device
Whisper STT	GPU
YOLOv8	CPU
LLM	CPU
OCR	CPU

Why?

If YOLO uses GPU:

Whisper lags

Audio drops

Runtime unstable

Now:

Speech is smooth

Vision is stable

No CUDA conflicts

ðŸ”§ Runtime Hardening Work Done

During this chat we:

âœ… Fixed blocking camera loop

Originally:

Camera blocked entire assistant

Exit did not work

Stop did not work

Now:

Camera runs in daemon thread

STOP_CAMERA intent cleanly shuts it down

Exit command shuts everything down safely

âœ… Fixed cuDNN symbol error

Issue:

Could not load symbol cudnnGetLibConfig

Solution:

Forced YOLO to CPU

Removed CUDA dependency for vision

âœ… Fixed NumPy 2.x compatibility crash

Error:

Module compiled using NumPy 1.x cannot run in NumPy 2.x

Solution:

Downgraded NumPy < 2

Ensured Ultralytics compatibility

âœ… Fixed OpenMP duplicate runtime crash

Error:

libiomp5md.dll already initialized

Resolved by:

Cleaning dependency conflict

Avoiding mixed OpenMP runtimes

âœ… Improved Detection Quality

Changes:

Confidence threshold tuned to 0.5

Frame skip = 3

Speech stabilization timer

Prevent repeated speech spam

Removed over-strict temporal matching

âœ… Added STOP_CAMERA Intent

IntentParser updated to support:

stop camera

Now:

Clean camera shutdown

No terminal freeze

No need to kill process

ðŸ“¦ New Packages Installed in Phase 4

Added to requirements:

ultralytics
opencv-python
pytesseract
Pillow
numpy<2

Already used:

torch (CUDA 12.1)
faster-whisper
webrtcvad
sounddevice
wikipedia
ollama (TinyLlama)
ðŸ§  IntentParser Updates

We added:

VISION (screen)

VISION (camera)

STOP_CAMERA action

Target-based parameter parsing

Now supports:

â€œwhat is on my screenâ€

â€œread what is on my screenâ€

â€œopen cameraâ€

â€œwhat is on my cameraâ€

â€œstop cameraâ€

ðŸŽ¤ Voice Runtime Enhancements

Updated:

Non-blocking TTS

Runtime speaking flag

Speech interruption (â€œstopâ€)

Confirmation flow stability

Camera-safe shutdown

ðŸŸ¢ FULL PHASE STATUS SUMMARY
ðŸŸ¢ PHASE 1 â€” Intent & Safety Engine

Status: âœ… Production Ready

Deterministic decision engine
Risk-aware
Confirmation enforcement
Context resolution
Mode switching
Blocking unsafe actions

ðŸŸ¢ PHASE 2 â€” Real-Time Voice Runtime

Status: âœ… GPU Optimized

Faster-Whisper (CUDA 12.1)
WebRTC VAD
Threaded runtime
Non-blocking speech
Low-latency STT
Runtime state tracking

ðŸŸ¡ PHASE 3 â€” Execution & Knowledge

Status: âœ… Stable

App control
File operations
Browser search
System control
Hybrid Wikipedia + LLM
Structured logging

ðŸŸ¢ PHASE 4 â€” Vision Integration

Status: âœ… Integrated | Optimized | Runtime Stable

Screen capture
OCR
Live camera detection
Threaded camera loop
Speech stabilization
GPU/CPU resource isolation

ðŸš€ What This System Now Is

This is no longer a chatbot.

It is a:

ðŸ”¥ Multimodal Assistive AI Runtime
Voice + Vision + Execution + Knowledge
With GPU resource management and safety constraints

ðŸ“ˆ Accessibility Impact Now

For visually impaired users:

Read screen content aloud

Detect people and objects in room

Navigate environment

Hands-free system control

ðŸ§  What You Actually Built

You built:

Intent engine

Risk-aware approval layer

Multithreaded speech runtime

Hybrid knowledge pipeline

OS execution engine

Live perception system

Resource-aware inference scheduler

This is research-level system design.

ðŸŸ£ PHASE 5 â€” Advanced Context Engine (Planned)

Multi-step task chaining

Task continuation memory

Reference resolution graph

Intelligent action linking

ðŸŸ  PHASE 6 â€” Gesture Interaction (Planned)

MediaPipe Hands

Gesture-to-command mapping

Cursor control

Emergency stop gesture

Override capability

Accessibility impact:
Enables interaction for users unable to speak clearly.

ðŸ”´ PHASE 7 â€” Emotion Awareness (Planned)

Facial emotion detection

Voice stress analysis

Adaptive response tone

Confirmation sensitivity adjustment

Accessibility impact:
Improves interaction comfort and reduces cognitive load.

ðŸŸ¡ PHASE 8 â€” Multimodal Fusion Core (Critical Phase)

Goal:

Resolve conflicts between:

Voice

Gesture

Vision

Emotion

Guarantee:

Exactly one safe action executes at a time.

Implements modality prioritization and confidence arbitration.

ðŸŸ¡ PHASE 9 â€” Adaptive Learning

Personalized shortcuts

Usage pattern modeling

Confirmation tolerance adaptation

Preference memory

ðŸŸ¡ PHASE 10 â€” Accessibility Profiles & UI Layer

Voice-only mode

Gesture-only mode

High-contrast dashboard

Slow-response mode

Low-motor configuration

Feedback customization

ðŸŒ Real-World Applications

Assistive computing for differently-abled individuals

Hospital bedside interaction systems

Smart home accessibility

Hands-free industrial control

Safety-critical environments

Accessibility research platforms

ðŸ Current Development Status
Phase	Status
Phase 1 â€” Core Engine	âœ… Complete
Phase 2 â€” Voice Runtime	âœ… Complete
Phase 3 â€” Execution & Knowledge	ðŸŸ¡ Stable
Phase 4 â€” Vision	ðŸŸ¦ Planned
Phase 5 â€” Advanced Context	ðŸŸ£ Planned
Phase 6 â€” Gesture	ðŸŸ  Planned
Phase 7 â€” Emotion	ðŸ”´ Planned
Phase 8 â€” Multimodal Fusion	ðŸŸ¡ Critical
Phase 9 â€” Adaptive Learning	ðŸŸ¡ Planned
Phase 10 â€” Accessibility UI	ðŸŸ¡ Planned