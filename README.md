Project Vision

This project is an AI-powered multimodal personal assistant tailored for differently-abled individuals, including:

Visually impaired users

Motor-disabled users

Speech-impaired users

Users with limited mobility or coordination

Users requiring adaptive interaction models

The system enables natural, intuitive, and safe interaction with both digital and physical environments using:

ðŸŽ¤ Voice input and output

âœ‹ Gesture recognition

ðŸ‘ Vision-based perception

ðŸ˜Š Emotion awareness

ðŸ§  Context-aware reasoning

ðŸŽ¯ Core Objective

To bridge the gap between human intent and machine execution by building a deterministic, safety-aware, multimodal assistive intelligence system that:

Reduces dependency on traditional input devices (keyboard, mouse)

Provides accessible computing interfaces

Ensures safe automation through confirmation safeguards

Maintains contextual awareness across interactions

Supports adaptive interaction based on user capability

ðŸ” Foundational Design Principles

The system is built on the following non-negotiable guarantees:

Deterministic decision logic

Explicit confirmation for high-risk actions

Single-action safe execution model

Context retention for multi-step interaction

Accessibility-first design

Safety over speed

Structured audit logging for transparency

ðŸ— High-Level System Architecture
Voice / Gesture / Vision / Emotion Inputs
                  â†“
           Intent Parsing Layer
                  â†“
           Context Memory
                  â†“
           Safety Evaluation
                  â†“
           Fusion Engine
                  â†“
           Decision Routing
                  â†“
           Execution Engine
                  â†“
           Adaptive User Feedback

KRISHNA/
â”œâ”€â”€ __pycache__/
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ context_memory.py
â”‚   â”œâ”€â”€ fusion_engine.py
â”‚   â”œâ”€â”€ intent_parser.py
â”‚   â”œâ”€â”€ intent_schema.py
â”‚   â”œâ”€â”€ mode_manager.py
â”‚   â”œâ”€â”€ response_model.py
â”‚   â”œâ”€â”€ safety_engine.py
â”‚   â””â”€â”€ safety_rules.py
â”‚
â”œâ”€â”€ execution/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   â”œâ”€â”€ windows_app.py
â”‚   â”‚   â”œâ”€â”€ windows_browser.py
â”‚   â”‚   â”œâ”€â”€ windows_file.py
â”‚   â”‚   â”œâ”€â”€ windows_keyboard.py
â”‚   â”‚   â””â”€â”€ windows_system.py
â”‚   â”‚
â”‚   â”œâ”€â”€ vision/
â”‚   â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ camera_detector.py
â”‚   â”‚   â”œâ”€â”€ ocr_engine.py
â”‚   â”‚   â”œâ”€â”€ screen_capture.py
â”‚   â”‚   â”œâ”€â”€ vision_executor.py
â”‚   â”‚   â””â”€â”€ app_control.py
â”‚   â”‚
â”‚   â”œâ”€â”€ dispatcher.py
â”‚   â”œâ”€â”€ execution_logger.py
â”‚   â”œâ”€â”€ executor.py
â”‚   â”œâ”€â”€ execution.py
â”‚   â”œâ”€â”€ file_ops.py
â”‚   â””â”€â”€ keyboard_mouse.py
â”‚
â”œâ”€â”€ knowledge/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ knowledge_engine.py
â”‚   â””â”€â”€ llm_engine.py
â”‚
â”œâ”€â”€ router/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â””â”€â”€ decision_router.py
â”‚
â”œâ”€â”€ utility/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â””â”€â”€ utility_engine.py
â”‚
â”œâ”€â”€ voice/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ assistant_runtime.py
â”‚   â”œâ”€â”€ mic_stream.py
â”‚   â”œâ”€â”€ stt.py
â”‚   â”œâ”€â”€ tts.py
â”‚   â”œâ”€â”€ vad.py
â”‚   â”œâ”€â”€ voice_loop.py
â”‚   â””â”€â”€ wakeword.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ test_execution_vision.py
â”‚   â”œâ”€â”€ test_router_vision.py
â”‚   â”œâ”€â”€ test_vision_parser.py
â”‚   â”œâ”€â”€ test_context.py
â”‚   â”œâ”€â”€ test_execution_hardening_manual.py
â”‚   â”œâ”€â”€ test_executor.py
â”‚   â”œâ”€â”€ test_knowledge.py
â”‚   â”œâ”€â”€ test_llm_direct.py
â”‚   â”œâ”€â”€ test_output.py
â”‚   â”œâ”€â”€ test_parser.py
â”‚   â”œâ”€â”€ test_phase2_pipeline.py
â”‚   â”œâ”€â”€ test_response.py
â”‚   â”œâ”€â”€ test_router.py
â”‚   â”œâ”€â”€ test_safety.py
â”‚   â”œâ”€â”€ test_utility.py
â”‚   â”œâ”€â”€ tests_execution.py
â”‚   â””â”€â”€ tests_phase2.py
â”‚
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ demo_full_pipeline.py
â”œâ”€â”€ demo_phase2.py
â”œâ”€â”€ run_phase2_voice.py
â”‚
â”œâ”€â”€ check_active_window.py
â”œâ”€â”€ manual_vision_test.py
â”œâ”€â”€ direct_record_test.py
â”œâ”€â”€ mic_test.py
â”œâ”€â”€ raw_stt_stream_test.py
â”œâ”€â”€ raw_stt_test.py
â”‚
â”œâ”€â”€ intent_parser_reference.py
â”œâ”€â”€ INTENT_PATTERNS_ANALYSIS.json
â”‚
â”œâ”€â”€ execution_logs.json
â”‚
â”œâ”€â”€ test_results.txt
â”œâ”€â”€ test_results_clean.txt
â”‚
â””â”€â”€ yolov8n.pt


ðŸŸ¢ PHASE 1 â€” Core Intent & Safety Engine
Status: âœ… Production Ready
Mission

Build a deterministic, risk-aware decision engine that safely interprets user intent.

Implemented

Structured Intent modeling (confidence, risk level, confirmation flags)

Natural language parsing with rule-based + contextual handling

Context memory (reference resolution: â€œclose itâ€)

Mode-based interaction control

Safety engine with risk escalation

Confirmation enforcement for high-risk actions

Latency tracking

Deterministic approval flow

Unknown input blocking

Stability

Fully tested

No blocking loops

Fully integrated with voice + vision

ðŸŸ¢ PHASE 2 â€” Real-Time Voice Runtime
Status: âœ… GPU Optimized | Stable
Mission

Enable fully hands-free interaction via real-time speech.

Technologies

Faster-Whisper (CUDA 12.1)

PyTorch GPU

WebRTC VAD

SoundDevice

Threaded architecture

Non-blocking TTS (PowerShell-based)

Implemented Capabilities

16kHz microphone streaming

Silence-based segmentation

Noise filtering

Speech interruption (â€œstopâ€)

Confirmation handling

Runtime state tracking

Clean thread shutdown

Stability

GPU dedicated to Whisper

No speech lag

No thread deadlocks

Clean exit behavior

ðŸŸ¡ PHASE 3 â€” Execution & Knowledge
Status: âœ… Stable | Production-Functional
Mission

Safely connect approved intents to real-world actions.

3.1 Execution Engine

Windows app control

File operations

Browser search

System control

Safe shutdown / restart

Structured execution logging

Confirmation enforcement

3.2 Utility Engine

Mathematical calculations

System time queries

Lightweight deterministic logic

3.3 Hybrid Knowledge Engine

Wikipedia API for factual queries

TinyLlama (Ollama) for reasoning

Clean summarization (max 2 sentences)

Prompt hardening

No conversational filler

ðŸŸ¢ PHASE 4 â€” Vision Integration
Status:

ðŸŸ¢ Core Functional
ðŸŸ¢ Runtime Stable
ðŸŸ¡ Intelligence-Level Improvements Pending
ðŸ”µ Production Hardening In Progress

ðŸŽ¯ Mission of Phase 4

Transform the assistant from:

Voice-driven OS controller

Into:

Multimodal perceptual assistant capable of understanding and narrating the visual world.

Phase 4 enables environmental awareness.

ðŸ— WHAT HAS ACTUALLY BEEN ENGINEERED
1ï¸âƒ£ Screen Vision
File:

execution/vision/screen_capture.py

Capabilities

Full screen capture

Windows-compatible

Integrated into OCR pipeline

Non-blocking execution

2ï¸âƒ£ OCR Text Reading
File:

execution/vision/ocr_engine.py

Capabilities

Text extraction via Tesseract

Image preprocessing (grayscale, threshold)

Noise cleanup

Speech-friendly formatting

Handles empty results safely

Supported Commands

â€œread what is on my screenâ€

â€œwhat is on my screenâ€

3ï¸âƒ£ Live Camera Object Detection
File:

execution/vision/camera_detector.py

Implemented Features

YOLOv8 inference

CPU-based detection (GPU preserved for Whisper)

Frame skipping (performance tuning)

Confidence filtering (>= 0.5)

Bounding box drawing

Non-blocking daemon thread loop

Stable speech emission (2s interval)

Clean STOP_CAMERA intent

Exit-safe shutdown

Terminal + voice narration

Concurrent voice + vision execution

ðŸ§  Critical Architectural Decision
GPU Resource Isolation Strategy

Device: RTX 2050 (4GB VRAM)

Component	Device
Whisper STT	GPU
YOLO	CPU
OCR	CPU
LLM	CPU
Why?

If YOLO used GPU:

cuDNN conflicts

CUDA memory contention

Audio lag

Runtime instability

Current Result:

Smooth speech

Stable vision

No CUDA crashes

No cuDNN symbol errors

This is production-grade resource isolation.

ðŸ”§ Runtime Hardening Completed in Phase 4
âœ… Fixed Blocking Camera Loop

Camera moved to daemon thread

STOP_CAMERA intent implemented

Exit safely shuts down all threads

âœ… Fixed cuDNN Symbol Error

Forced YOLO to CPU

âœ… Fixed NumPy 2.x Crash

Pinned numpy < 2

Ensured Ultralytics compatibility

âœ… Fixed OpenMP Duplicate Runtime Crash

Cleaned dependency conflicts

âœ… Detection Stability Improvements

Frame skip = 3

Confidence threshold tuned

Speech stabilization interval

Removed over-aggressive temporal locking

ðŸš€ CURRENT CAPABILITIES (Phase 4)

The system can:

âœ” Detect objects in real-time
âœ” Narrate scene objects
âœ” Read screen text
âœ” Accept commands during camera mode
âœ” Stop camera safely
âœ” Exit safely
âœ” Maintain concurrent voice + vision

This is a stable multimodal runtime.

ðŸŸ¡ WHAT IS NOT YET PRODUCTION-LEVEL

Currently:

Object detection is implemented.

But production assistive AI requires:

Understanding, tracking, and contextual awareness.

ðŸ”´ PHASE 4 MUST EVOLVE INTO

To reach production-grade intelligence, Phase 4 must add:

1ï¸âƒ£ Object Tracking

Current:
YOLO detects each frame independently.

Missing:

Persistent object identity

Entry/exit detection

Motion tracking

Upgrade:
Add ByteTrack or DeepSORT.

Enables:

â€œA person entered the room.â€

â€œThe phone disappeared.â€

Stable bounding boxes

2ï¸âƒ£ Scene Understanding

Current:
â€œI see 1 person, 1 phone.â€

Production:
â€œA person is holding a phone.â€
â€œThere is a laptop on the table.â€

Requires:

Spatial reasoning

Bounding box relationship logic

Lightweight Vision-Language Model (optional)

3ï¸âƒ£ Smart Object Filtering

Add priority whitelist:

person

chair

door

phone

vehicle

obstacles

Reduce irrelevant detections (fork, tie, toothbrush).

4ï¸âƒ£ Spatial Awareness

Add:

Left/center/right zone detection

Distance estimation

Object proximity awareness

Enables:
â€œPerson on your left.â€
â€œPhone is in the center.â€

5ï¸âƒ£ Event Detection

Add scene memory:

Object appeared

Object disappeared

Sudden movement

Fall detection

6ï¸âƒ£ Multimodal Fusion

Currently:
Voice and vision are parallel.

Future:
Voice queries vision.

Example:
User: â€œWhere is my phone?â€
System:

Searches frame

Determines position

Responds with spatial guidance

7ï¸âƒ£ Environmental Modes

Add:

Passive narration

Alert mode

Safety mode

Safety mode:

Fall detection

Fire/smoke detection

Obstacle alerts

ðŸŸ£ PHASE 5 â€” Advanced Context Engine (Planned)

Multi-step task chaining

Task continuation memory

Reference resolution graph

Intelligent action linking

ðŸŸ  PHASE 6 â€” Gesture Interaction (Planned)

MediaPipe Hands

Gesture-to-command mapping

Cursor control

Emergency stop gesture

Override capability

Accessibility impact:
Enables interaction for users unable to speak clearly.

ðŸ”´ PHASE 7 â€” Emotion Awareness (Planned)

Facial emotion detection

Voice stress analysis

Adaptive response tone

Confirmation sensitivity adjustment

Accessibility impact:
Improves interaction comfort and reduces cognitive load.

ðŸŸ¡ PHASE 8 â€” Multimodal Fusion Core (Critical Phase)

Goal:

Resolve conflicts between:

Voice

Gesture

Vision

Emotion

Guarantee:

Exactly one safe action executes at a time.

Implements modality prioritization and confidence arbitration.

ðŸŸ¡ PHASE 9 â€” Adaptive Learning

Personalized shortcuts

Usage pattern modeling

Confirmation tolerance adaptation

Preference memory

ðŸŸ¡ PHASE 10 â€” Accessibility Profiles & UI Layer

Voice-only mode

Gesture-only mode

High-contrast dashboard

Slow-response mode

Low-motor configuration

Feedback customization

ðŸŒ Real-World Applications

Assistive computing for differently-abled individuals

Hospital bedside interaction systems

Smart home accessibility

Hands-free industrial control

Safety-critical environments

Accessibility research platforms

ðŸ Current Development Status
Phase	Status
Phase 1 â€” Core Engine	âœ… Complete
Phase 2 â€” Voice Runtime	âœ… Complete
Phase 3 â€” Execution & Knowledge	ðŸŸ¡ Stable
Phase 4 â€” Vision	ðŸŸ¦ Planned
Phase 5 â€” Advanced Context	ðŸŸ£ Planned
Phase 6 â€” Gesture	ðŸŸ  Planned
Phase 7 â€” Emotion	ðŸ”´ Planned
Phase 8 â€” Multimodal Fusion	ðŸŸ¡ Critical
Phase 9 â€” Adaptive Learning	ðŸŸ¡ Planned
Phase 10 â€” Accessibility UI	ðŸŸ¡ Planned