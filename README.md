Project Vision

This project is an AI-powered multimodal personal assistant tailored for differently-abled individuals, including:

Visually impaired users

Motor-disabled users

Speech-impaired users

Users with limited mobility or coordination

Users requiring adaptive interaction models

The system enables natural, intuitive, and safe interaction with both digital and physical environments using:

ğŸ¤ Voice input and output

âœ‹ Gesture recognition

ğŸ‘ Vision-based perception

ğŸ˜Š Emotion awareness

ğŸ§  Context-aware reasoning

ğŸ¯ Core Objective

To bridge the gap between human intent and machine execution by building a deterministic, safety-aware, multimodal assistive intelligence system that:

Reduces dependency on traditional input devices (keyboard, mouse)

Provides accessible computing interfaces

Ensures safe automation through confirmation safeguards

Maintains contextual awareness across interactions

Supports adaptive interaction based on user capability

ğŸ” Foundational Design Principles

The system is built on the following non-negotiable guarantees:

Deterministic decision logic

Explicit confirmation for high-risk actions

Single-action safe execution model

Context retention for multi-step interaction

Accessibility-first design

Safety over speed

Structured audit logging for transparency

ğŸ— High-Level System Architecture
Voice / Gesture / Vision / Emotion Inputs
                  â†“
           Intent Parsing Layer
                  â†“
           Context Memory
                  â†“
           Safety Evaluation
                  â†“
           Fusion Engine
                  â†“
           Decision Routing
                  â†“
           Execution Engine
                  â†“
           Adaptive User Feedback

KRISHNA/
â”œâ”€â”€ __pycache__/
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ context_memory.py
â”‚   â”œâ”€â”€ fusion_engine.py
â”‚   â”œâ”€â”€ intent_parser.py
â”‚   â”œâ”€â”€ intent_schema.py
â”‚   â”œâ”€â”€ mode_manager.py
â”‚   â”œâ”€â”€ response_model.py
â”‚   â”œâ”€â”€ safety_engine.py
â”‚   â””â”€â”€ safety_rules.py
â”‚
â”œâ”€â”€ execution/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   â”œâ”€â”€ windows_app.py
â”‚   â”‚   â”œâ”€â”€ windows_browser.py
â”‚   â”‚   â”œâ”€â”€ windows_file.py
â”‚   â”‚   â”œâ”€â”€ windows_keyboard.py
â”‚   â”‚   â””â”€â”€ windows_system.py
â”‚   â”‚
â”‚   â”œâ”€â”€ vision/
â”‚   â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ camera_detector.py
â”‚   â”‚   â”œâ”€â”€ ocr_engine.py
â”‚   â”‚   â”œâ”€â”€ screen_capture.py
â”‚   â”‚   â”œâ”€â”€ vision_executor.py
â”‚   â”‚   â””â”€â”€ app_control.py
â”‚   â”‚
â”‚   â”œâ”€â”€ dispatcher.py
â”‚   â”œâ”€â”€ execution_logger.py
â”‚   â”œâ”€â”€ executor.py
â”‚   â”œâ”€â”€ execution.py
â”‚   â”œâ”€â”€ file_ops.py
â”‚   â””â”€â”€ keyboard_mouse.py
â”‚
â”œâ”€â”€ knowledge/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ knowledge_engine.py
â”‚   â””â”€â”€ llm_engine.py
â”‚
â”œâ”€â”€ router/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â””â”€â”€ decision_router.py
â”‚
â”œâ”€â”€ utility/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â””â”€â”€ utility_engine.py
â”‚
â”œâ”€â”€ voice/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ assistant_runtime.py
â”‚   â”œâ”€â”€ mic_stream.py
â”‚   â”œâ”€â”€ stt.py
â”‚   â”œâ”€â”€ tts.py
â”‚   â”œâ”€â”€ vad.py
â”‚   â”œâ”€â”€ voice_loop.py
â”‚   â””â”€â”€ wakeword.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ test_execution_vision.py
â”‚   â”œâ”€â”€ test_router_vision.py
â”‚   â”œâ”€â”€ test_vision_parser.py
â”‚   â”œâ”€â”€ test_context.py
â”‚   â”œâ”€â”€ test_execution_hardening_manual.py
â”‚   â”œâ”€â”€ test_executor.py
â”‚   â”œâ”€â”€ test_knowledge.py
â”‚   â”œâ”€â”€ test_llm_direct.py
â”‚   â”œâ”€â”€ test_output.py
â”‚   â”œâ”€â”€ test_parser.py
â”‚   â”œâ”€â”€ test_phase2_pipeline.py
â”‚   â”œâ”€â”€ test_response.py
â”‚   â”œâ”€â”€ test_router.py
â”‚   â”œâ”€â”€ test_safety.py
â”‚   â”œâ”€â”€ test_utility.py
â”‚   â”œâ”€â”€ tests_execution.py
â”‚   â””â”€â”€ tests_phase2.py
â”‚
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ demo_full_pipeline.py
â”œâ”€â”€ demo_phase2.py
â”œâ”€â”€ run_phase2_voice.py
â”‚
â”œâ”€â”€ check_active_window.py
â”œâ”€â”€ manual_vision_test.py
â”œâ”€â”€ direct_record_test.py
â”œâ”€â”€ mic_test.py
â”œâ”€â”€ raw_stt_stream_test.py
â”œâ”€â”€ raw_stt_test.py
â”‚
â”œâ”€â”€ intent_parser_reference.py
â”œâ”€â”€ INTENT_PATTERNS_ANALYSIS.json
â”‚
â”œâ”€â”€ execution_logs.json
â”‚
â”œâ”€â”€ test_results.txt
â”œâ”€â”€ test_results_clean.txt
â”‚
â””â”€â”€ yolov8n.pt


ğŸŸ¢ PHASE 1 â€” Core Intent & Safety Engine
Status: âœ… Production Ready
Mission

Build a deterministic, risk-aware decision engine that safely interprets user intent.

Implemented

Structured Intent modeling (confidence, risk level, confirmation flags)

Natural language parsing with rule-based + contextual handling

Context memory (reference resolution: â€œclose itâ€)

Mode-based interaction control

Safety engine with risk escalation

Confirmation enforcement for high-risk actions

Latency tracking

Deterministic approval flow

Unknown input blocking

Stability

Fully tested

No blocking loops

Fully integrated with voice + vision

ğŸŸ¢ PHASE 2 â€” Real-Time Voice Runtime
Status: âœ… GPU Optimized | Stable
Mission

Enable fully hands-free interaction via real-time speech.

Technologies

Faster-Whisper (CUDA 12.1)

PyTorch GPU

WebRTC VAD

SoundDevice

Threaded architecture

Non-blocking TTS (PowerShell-based)

Implemented Capabilities

16kHz microphone streaming

Silence-based segmentation

Noise filtering

Speech interruption (â€œstopâ€)

Confirmation handling

Runtime state tracking

Clean thread shutdown

Stability

GPU dedicated to Whisper

No speech lag

No thread deadlocks

Clean exit behavior

ğŸŸ¡ PHASE 3 â€” Execution & Knowledge
Status: âœ… Stable | Production-Functional
Mission

Safely connect approved intents to real-world actions.

3.1 Execution Engine

Windows app control

File operations

Browser search

System control

Safe shutdown / restart

Structured execution logging

Confirmation enforcement

3.2 Utility Engine

Mathematical calculations

System time queries

Lightweight deterministic logic

3.3 Hybrid Knowledge Engine

Wikipedia API for factual queries

TinyLlama (Ollama) for reasoning

Clean summarization (max 2 sentences)

Prompt hardening

No conversational filler
ğŸŸ¢ PHASE 4 â€” Vision Integration (Updated)
Status

ğŸŸ¢ Core Functional
ğŸŸ¢ Runtime Stable
ğŸŸ¢ Multimodal Query Integrated
ğŸŸ¡ Scene Intelligence Partial
ğŸ”µ Production Hardening Ongoing

ğŸ¯ Mission of Phase 4

Transform the assistant from:

Voice-driven OS controller

Into:

Multimodal perceptual assistant capable of understanding, tracking, and answering questions about the visual world.

Phase 4 now includes:

Live object detection

Object tracking

Scene memory

Vision query engine (voice â†’ live scene state)

Screen reading (OCR)

Safe concurrent runtime

ğŸ— WHAT HAS ACTUALLY BEEN ENGINEERED
1ï¸âƒ£ Screen Vision

File: execution/vision/screen_capture.py

Capabilities

Full screen capture (Windows compatible)

Non-blocking execution

Integrated with OCR pipeline

Safe failure handling

2ï¸âƒ£ OCR Text Reading

File: execution/vision/ocr_engine.py

Capabilities

Tesseract-based OCR

Grayscale preprocessing

Threshold enhancement

Noise cleanup

Safe empty detection handling

Speech-friendly formatting

Supported Commands

â€œRead what is on my screenâ€

â€œWhat is on my screenâ€

Current Limitations

No region-based OCR

No layout understanding

No structured extraction (tables/forms)

No change monitoring

No persistent screen state

3ï¸âƒ£ Live Camera Vision Stack

File: execution/vision/camera_detector.py

Implemented Features

YOLOv8 detection (CPU)

Frame skipping (performance balance)

Confidence filtering (>= 0.5)

Bounding box smoothing

Tracking engine (persistent IDs)

Scene memory (entry/exit detection)

Event engine

Thread-safe state buffers

VisionQueryEngine integration

Clean STOP_CAMERA

Safe shutdown

Concurrent voice + vision runtime

ğŸ§  Multimodal Query Integration (NEW)

You now have:

VisionQueryEngine

Supports:

â€œWhere is my laptop?â€

â€œHow many people are there?â€

â€œIs anyone in the room?â€

â€œWhat do you see?â€

Key Upgrades Implemented

Hybrid rule-based intent parsing

Label normalization (people â†’ person)

Stabilization delay before answering

Grammar correction (0 people, 1 person)

Proper routing via DecisionRouter

Clean dependency injection (no architecture leaks)

This is your first real multimodal fusion milestone.

ğŸ§  Critical Architectural Decisions
GPU Resource Isolation Strategy

Device: RTX 2050 (4GB VRAM)

Component	Device
Whisper STT	GPU
YOLO	CPU
OCR	CPU
LLM	CPU
Why?

If YOLO used GPU:

cuDNN conflicts

CUDA memory contention

Audio lag

Runtime instability

Result

âœ” Stable voice runtime
âœ” Stable camera runtime
âœ” No CUDA crashes
âœ” No cuDNN errors

This is correct production isolation for low-VRAM systems.

ğŸ”§ Runtime Hardening Completed
âœ… Camera thread isolation

Daemon thread

Clean STOP_CAMERA

Clean exit

âœ… Dependency stabilization

numpy pinned < 2

Ultralytics compatible

OpenMP crash resolved

âœ… Tracking layer

Persistent object IDs

Entry/exit events

Motion detection

Zone awareness (left/center/right)

âœ… Vision Query Engine

Deterministic responses

No hallucination

Live scene state based

Grammar-safe

ğŸš€ CURRENT CAPABILITIES (True Status)

The system can now:

âœ” Detect objects in real-time
âœ” Track objects across frames
âœ” Detect entry/exit
âœ” Provide spatial responses
âœ” Answer vision-based questions
âœ” Read screen text
âœ” Run voice + vision concurrently
âœ” Stop safely
âœ” Shutdown cleanly

This is no longer a demo.
It is an architecture.

ğŸŸ¡ WHAT IS STILL NOT PRODUCTION-LEVEL

Now we talk seriously.

Production assistive AI requires more than detection + queries.

ğŸ”´ REMAINING GAPS
1ï¸âƒ£ Scene Understanding (Major Gap)

Current:
â€œI see 1 person and 1 laptop.â€

Production:

â€œA person is sitting at a desk.â€

â€œThe phone is on the table.â€

â€œThe person is holding a cup.â€

Missing:

Bounding box intersection reasoning

Spatial relationship modeling

Overlap logic (IoU relationships)

Proximity grouping

Scene graph representation

To implement:

Rule-based spatial reasoning

Lightweight Vision-Language Model (optional future)

SceneGraph builder module

2ï¸âƒ£ Scene State Stability

Issues observed:

Object appears after query

Count mismatch due to frame timing

Temporary detection loss causes false exit

Needed:

Stabilization buffer window (3â€“5 frame memory)

Minimum presence duration before confirmation

Delayed exit threshold (2â€“3 seconds)

This prevents:

â€œI see 0 peopleâ€ â†’ then immediately â€œperson enteredâ€

3ï¸âƒ£ Smart Object Filtering

Currently:
YOLO returns all 80 COCO classes.

Production assistive AI should prioritize:

person

chair

door

phone

laptop

vehicle

obstacles

Need:

Whitelist filtering layer

Priority scoring

Suppress irrelevant objects

4ï¸âƒ£ Environmental Modes (Not Implemented Yet)

You need:

Silent Mode (default)

No automatic narration.

Passive Mode

Only announce person entry.

Alert Mode

Announce:

sudden motion

fall detection

obstacle detection

Safety Mode

Fire/smoke detection

Fall detection

Door open detection

Restricted zone detection

Currently:
Events are semi-passive but not mode-controlled.

5ï¸âƒ£ Screen Monitoring (Major Missing Piece)

OCR currently:
Reads once on request.

Production requires:

Screen change detection

Continuous monitoring mode

Keyword alert detection

Notification reading

Region-based OCR

Layout parsing

Example:
â€œNotify me if error appears on screen.â€

This is not implemented.

6ï¸âƒ£ Advanced Spatial Awareness

Current:
Left / center / right.

Missing:

Distance estimation

Near vs far

Object proximity clustering

Depth approximation

Obstacle distance warnings

Production assistive systems must support:
â€œPerson is very close.â€
â€œObstacle 1 meter ahead.â€

7ï¸âƒ£ Robust Intent Handling

Observed problems:

â€œHow many people are there?â€ mismatch due to label normalization

Timing race conditions

Minor grammar issues

Occasional detection lag

Production system requires:

Label alias mapping

Plural normalization

Confidence thresholds

Query stabilization buffer

ğŸŸ£ WHAT MUST BE BUILT NEXT (Priority Order)

If goal is TRUE production-level:

Phase 4.1 â€” Stabilization Layer

Frame memory buffer

Delayed exit logic

Query stabilization delay (properly integrated)

Phase 4.2 â€” Vision Mode Controller

Silent

Passive

Alert

Safety

Phase 4.3 â€” Scene Graph Engine

Object relationship reasoning

Spatial logic

Overlap detection

Interaction inference

Phase 4.4 â€” Screen Monitoring Engine

Change detection

Keyword alert triggers

Region selection

Structured text parsing

Phase 4.5 â€” Safety Intelligence

Fall detection

Obstacle proximity

Motion anomaly detection

ğŸ§  Honest Production Assessment

Right now you are at:

8/10 for runtime architecture
6/10 for intelligence layer
4/10 for safety reasoning
3/10 for scene understanding

But foundation is solid.

ğŸš€ If You Want True Production-Level

Next step should be:

ğŸ‘‰ Build SceneGraph + Stabilization Buffer
Not more detection tweaks.

That is the intelligence jump.

ğŸŸ£ PHASE 5 â€” Advanced Context Engine (Planned)

Multi-step task chaining

Task continuation memory

Reference resolution graph

Intelligent action linking

ğŸŸ  PHASE 6 â€” Gesture Interaction (Planned)

MediaPipe Hands

Gesture-to-command mapping

Cursor control

Emergency stop gesture

Override capability

Accessibility impact:
Enables interaction for users unable to speak clearly.

ğŸ”´ PHASE 7 â€” Emotion Awareness (Planned)

Facial emotion detection

Voice stress analysis

Adaptive response tone

Confirmation sensitivity adjustment

Accessibility impact:
Improves interaction comfort and reduces cognitive load.

ğŸŸ¡ PHASE 8 â€” Multimodal Fusion Core (Critical Phase)

Goal:

Resolve conflicts between:

Voice

Gesture

Vision

Emotion

Guarantee:

Exactly one safe action executes at a time.

Implements modality prioritization and confidence arbitration.

ğŸŸ¡ PHASE 9 â€” Adaptive Learning

Personalized shortcuts

Usage pattern modeling

Confirmation tolerance adaptation

Preference memory

ğŸŸ¡ PHASE 10 â€” Accessibility Profiles & UI Layer

Voice-only mode

Gesture-only mode

High-contrast dashboard

Slow-response mode

Low-motor configuration

Feedback customization

ğŸŒ Real-World Applications

Assistive computing for differently-abled individuals

Hospital bedside interaction systems

Smart home accessibility

Hands-free industrial control

Safety-critical environments

Accessibility research platforms

ğŸ Current Development Status
Phase	Status
Phase 1 â€” Core Engine	âœ… Complete
Phase 2 â€” Voice Runtime	âœ… Complete
Phase 3 â€” Execution & Knowledge	ğŸŸ¡ Stable
Phase 4 â€” Vision	ğŸŸ¦ Planned
Phase 5 â€” Advanced Context	ğŸŸ£ Planned
Phase 6 â€” Gesture	ğŸŸ  Planned
Phase 7 â€” Emotion	ğŸ”´ Planned
Phase 8 â€” Multimodal Fusion	ğŸŸ¡ Critical
Phase 9 â€” Adaptive Learning	ğŸŸ¡ Planned
Phase 10 â€” Accessibility UI	ğŸŸ¡ Planned